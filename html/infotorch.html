<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>infotorch API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>infotorch</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import torch
import torch.nn as nn
from torch.distributions import Normal

def skewness_fn(x, dim=1):
    &#39;&#39;&#39;Calculates skewness of data &#34;x&#34; along dimension &#34;dim&#34;.&#39;&#39;&#39;
    std, mean = torch.std_mean(x, dim)
    n = torch.Tensor([x.shape[dim]]).to(x.device)
    eps = 1e-6  # for stability

    sample_bias_adjustment = torch.sqrt(n * (n - 1)) / (n - 2)
    skewness = sample_bias_adjustment * (
        (torch.sum((x.T - mean.unsqueeze(dim).T).T.pow(3), dim) / n)
        / std.pow(3).clamp(min=eps)
    )
    return skewness


def kurtosis_fn(x, dim=1):
    &#39;&#39;&#39;Calculates kurtosis of data &#34;x&#34; along dimension &#34;dim&#34;.&#39;&#39;&#39;
    std, mean = torch.std_mean(x, dim)
    n = torch.Tensor([x.shape[dim]]).to(x.device)
    eps = 1e-6  # for stability

    sample_bias_adjustment = (n - 1) / ((n - 2) * (n - 3))
    kurtosis = sample_bias_adjustment * (
        (n + 1)
        * (
            (torch.sum((x.T - mean.unsqueeze(dim).T).T.pow(4), dim) / n)
            / std.pow(4).clamp(min=eps)
        )
        - 3 * (n - 1)
    )
    return kurtosis

def bimodality_index(x, dim=1):
    &#39;&#39;&#39;
    Used to detect bimodality (or multimodality) of dataset(s) given a tensor &#34;x&#34; containing the
    data and a dimension &#34;dim&#34; along which to calculate.  The logic behind this index is that a
    bimodal (or multimodal) distribution with light tails will have very low kurtosis, an asymmetric
    character, or both – all of which increase this index.  The smaller this value is the more
    likely the data are to follow a unimodal distribution.  As a rule: if return value ≤ 0.555
    (bimodal index for uniform distribution), the data are considered to follow a unimodal
    distribution. Otherwise, they follow a bimodal or multimodal distribution.
    &#39;&#39;&#39;
    # calculate standard deviation and mean of dataset(s)
    std, mean = torch.std_mean(x, dim)
    # get number of samples in dataset(s)
    n = torch.Tensor([x.shape[dim]]).to(x.device)
    eps = 1e-6  # for stability

    # calculate skewness:
    # repeating most of the skewness function here to avoid recomputation of standard devation and mean
    sample_bias_adjustment = torch.sqrt(n * (n - 1)) / (n - 2)
    skew = sample_bias_adjustment * (
        (torch.sum((x.T - mean.unsqueeze(dim).T).T.pow(3), dim) / n)
        / std.pow(3).clamp(min=eps)
    )

    # calculate kurtosis:
    # repeating most the kurtosis function here to avoid recomputation of standard devation and mean
    sample_bias_adjustment = (n - 1) / ((n - 2) * (n - 3))
    kurt = sample_bias_adjustment * (
        (n + 1)
        * (
            (torch.sum((z.T - mean.unsqueeze(dim).T).T.pow(4), dim) / n)
            / std.pow(4).clamp(min=eps)
        )
        - 3 * (n - 1)
    )

    # calculate bimodality index:
    BC = (skew.pow(2) + 1) / (kurt + 3 * ((n - 2).pow(2) / ((n - 2) * (n - 3))))

    return BC

def KernelDensityEstimate(
    data,
    x_tics=None,
    start=-9,
    end=9,
    kernel=Normal(loc=0, scale=1),
    bandwidth_adjustment=1,
    dim=1,
):
    &#39;&#39;&#39;Estimates the probability density function of a batch of data.&#39;&#39;&#39;
    # convert to positive index (important for unsqueezing)
    if dim &lt; 0:
        dim = len(data.shape) + dim
        if dim &gt; (len(data.shape) - 1) or dim &lt; 0:
            raise IndexError

    def kde_prob(
        data,
        x,
        dim=dim,
        kernel=Normal(loc=0, scale=1),
        bandwidth_adjustment=1,
    ):
        &#39;&#39;&#39;
        Returns the probability of the items in tensor &#39;x&#39; according to the PDF estimated by a KDE.
        This function is memory intensive.
        &#39;&#39;&#39;
        data = data.flatten(dim)
        n = data.shape[dim]
        silvermans_factor = ((4 * torch.std(data, dim).pow(5)) / (3 * n)).pow(1 / 5)

        bw = silvermans_factor * bandwidth_adjustment

        bw = bw.view(
            *bw.shape, *[1 for _ in range(len(data.shape) + 1 - len(bw.shape))]
        )
        a = data.unsqueeze(dim) - x.unsqueeze(1)
        a = a / bw
        a = kernel.log_prob(a)
        a = torch.exp(a)
        a = bw ** (-1) * a
        a = a.sum(dim=dim + 1)
        prob = a / n

        return prob

    if x_tics is None:
        assert start and end
        assert end &gt; start
        a = max(torch.min(data).item(), start)  # lower integration bound
        b = min(torch.max(data).item(), end)  # upper integration bound
        x_tics = torch.Tensor(np.linspace(a, b, steps)).to(data.device)
    else:
        assert isinstance(x_tics, torch.Tensor)
        x_tics = x_tics.to(data.device)
    x_tics.requires_grad = True
    kde_y_tics = kde_prob(
        data,
        x_tics,
        kernel=kernel,
        bandwidth_adjustment=bandwidth_adjustment,
    )
    return kde_y_tics

class Normal_Model(nn.Module):
    &#39;&#39;&#39;
    Example of a module for modeling a probability distribution. This is set up with all pieces
    required for use with the rest of this package. (initial parameters; as well as implimented
    constrain, forward, and log_prob methods)
    &#39;&#39;&#39;
    def __init__(
        self,
        init_mean: torch.Tensor = torch.Tensor([0]),
        init_std: torch.Tensor = torch.Tensor([1]),
    ):
        super(Normal_Model, self).__init__()
        self.mean = nn.Parameter(init_mean, requires_grad=True)
        self.std = nn.Parameter(init_std, requires_grad=True)
        # constant
        self.ln2p = nn.Parameter(
            torch.log(2 * torch.Tensor([torch.pi])), requires_grad=False
        )

    def constrain(self):
        &#39;&#39;&#39;
        Method to run on &#34;constrain&#34; step of training. Easiest method for optimization under
        constraint is Projection Optimization by simply clamping parameters to bounds after each
        update. This is certainly not the most efficent way, but it gets the job done.
        &#39;&#39;&#39;
        #  can&#39;t have negative standard deviation so lets prevent that:
        eps = 1e-6
        self.std.data = model.std.data.clamp(min=eps)

    def log_prob(self, x):
        &#39;&#39;&#39;
        Returns the log probability of the items in tensor &#39;x&#39; according to the probability
        distribution of the module.
        &#39;&#39;&#39;
        return (
            -torch.log(self.std.unsqueeze(-1))
            - (self.ln2p / 2)
            - ((x - self.mean.unsqueeze(-1)) / self.std.unsqueeze(-1)).pow(2) / 2
        )

    def forward(self, x):
        &#39;&#39;&#39;Returns the probability of the items in tensor &#39;x&#39; according to the probability distribution of the module.&#39;&#39;&#39;
        return self.log_prob(x).exp()

def MLE_Fit(model, data, dim=1, lr=5e-2, iters=250):
    &#39;&#39;&#39;
    Fits the parameters of the provided model to the provided data. Provided model must have
    implimented log_prob() and constrain() methods, and paraters set to some initial value.
    &#39;&#39;&#39;
    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)
    #  print(&#34;model parameters:&#34;, [x for x in model.parameters()])
    #  data = data.flatten(dim)
    for i in range(iters):
        nll = -torch.sum(model.log_prob(data))
        nll.backward()
        optimizer.step()
        optimizer.zero_grad()
        model.constrain()

def ECDF(x: torch.Tensor, dim: int = 0, reach_limits=True):
    &#34;&#34;&#34;
    set &#34;reach_limit&#34; to false to calculate ECDF in a way that will not include perfect 0 or 1.
    &#34;&#34;&#34;
    x = torch.sort(x.flatten(dim), dim=dim).values
    n = x.shape[-1]
    cum = torch.arange(1, n + 1).to(x.device) / (n + 1 - reach_limits)
    cum = cum.repeat(*x.shape[0:-1], 1)  # one for each univariate sample
    return x, cum

class Unbounded_Metalog_Model(nn.Module):
    def __init__(
        self,
        init_a: torch.Tensor = None,
    ):
        super(Unbounded_Metalog_Model, self).__init__()

        self.a = nn.Parameter(init_a, requires_grad=True)
        self.n = self.a.shape[-1]

        ### Define basis functions for QF (quantile function):
        def qg1(y, i):
            &#39;&#39;&#39;first basis function&#39;&#39;&#39;
            return torch.ones_like(y)

        def qg2(y, i):
            &#39;&#39;&#39;second basis function&#39;&#39;&#39;
            return torch.log(y / (1 - y))

        def qg3(y, i):
            &#39;&#39;&#39;third basis function&#39;&#39;&#39;
            return (y - 0.5) * torch.log(y / (1 - y))

        def qg4(y, i):
            &#39;&#39;&#39;fourth basis function&#39;&#39;&#39;
            return y - 0.5

        def qgj_odd(y, j):
            &#39;&#39;&#39;nth odd basis function (after third)&#39;&#39;&#39;
            j += 1
            assert (j % 2 != 0) and (j &gt;= 5)
            return (y - 0.5).pow((j - 1) / 2)

        def qgj_even(y, j):
            &#39;&#39;&#39;nth even basis function (after fourth)&#39;&#39;&#39;
            j += 1
            assert (j % 2 == 0) and (j &gt;= 6)
            return torch.log(y / (1 - y)) * (y - 0.5).pow(j / 2 - 1)

        # Start QF basis functions:
        self.qf_basis_functions = [qg1, qg2, qg3, qg4]
        # Additional inverse cdf basis functions as needed:
        self.qf_basis_functions = self.qf_basis_functions + [
            qgj_odd if x % 2 == 0 else qgj_even for x in range(self.n - 4)
        ]
        # Trim as needed:
        self.qf_basis_functions = self.qf_basis_functions[: self.n]

        ### Define basis functions for inverse PDF in terms of cumulative probability
        ### (^ derivative of quantile function):
        def dqg1(y, i):
            &#39;&#39;&#39;first basis function&#39;&#39;&#39;
            return torch.zeros_like(y)

        def dqg2(y, i):
            &#39;&#39;&#39;second basis function&#39;&#39;&#39;
            return 1 / (y * (1 - y))

        def dqg3(y, i):
            &#39;&#39;&#39;third basis function&#39;&#39;&#39;
            return (y - 1 / 2) / (y * (1 - y)) + torch.log(y / (1 - y))

        def dqg4(y, i):
            &#39;&#39;&#39;fourth basis function&#39;&#39;&#39;
            return torch.ones_like(y)

        def dqgj_odd(y, j):
            &#39;&#39;&#39;nth odd basis function (after third)&#39;&#39;&#39;
            j += 1
            assert (j % 2 != 0) and (j &gt;= 5)
            return ((j - 1) / 2) * (y - 1 / 2).pow((j - 3) / 2)

        def dqgj_even(y, j):
            &#39;&#39;&#39;nth even basis function (after fourth)&#39;&#39;&#39;
            j += 1
            assert (j % 2 == 0) and (j &gt;= 6)
            return (y - 1 / 2).pow(j / 2 - 1) / (y * (1 - y)) + (j / 2 - 1) * (
                y - 1 / 2
            ).pow(j / 2 - 2) * torch.log(y / (1 - y))

        # Start PDF basis functions:
        self.ipdf_basis_functions = [dqg1, dqg2, dqg3, dqg4]
        # Additional ipdf basis functions as needed:
        self.ipdf_basis_functions = self.ipdf_basis_functions + [
            dqgj_odd if x % 2 == 0 else dqgj_even for x in range(self.n - 4)
        ]
        # Trim as needed:
        self.ipdf_basis_functions = self.ipdf_basis_functions[: self.n]

    def constrain(self):
        &#39;&#39;&#39;Coefficients are unconstrained in this case.&#39;&#39;&#39;
        pass

    def quantile(self, y):
        &#39;&#39;&#39;
        Quantile of cumulative probability &#34;y&#34;.  (returns x-position of cumulative probability &#34;y&#34;.
        This is an inverse CDF)
        &#39;&#39;&#39;
        x_tics = sum(
            [
                self.a[:, idx].unsqueeze(-1) * f(y, idx)
                for idx, f in enumerate(self.qf_basis_functions)
            ]
        )
        return x_tics

    def prob_ito_cumprob(self, y):
        &#39;&#39;&#39;Probability density in terms of cumulative probability &#34;y&#34;.&#39;&#39;&#39;
        return sum(
            [
                self.a[:, idx].unsqueeze(-1) * f(y, idx)
                for idx, f in enumerate(self.ipdf_basis_functions)
            ]
        ).pow(
            -1
        )  # for reciprocal of sum of terms

    def prob(self, x, iters=64):
        &#39;&#39;&#39;
        Approximates probability density at a batch of tensors &#34;x&#34; by asymptotically bounded
        approach. There is currently no known closed-form inverse metalog.
        &#39;&#39;&#39;
        eps = 1e-7
        cum_y_guess = torch.ones_like(x) * 1 / 3

        lr = 1/3
        old_x_guess = self.quantile(cum_y_guess)  # initial
        old_diff = 0  # initial
        adj = torch.tensor([1]).to(x.device) / x.shape[1]  # initial
        for i in range(iters):
            cum_y_guess += adj
            x_guess = self.quantile(cum_y_guess)
            diff = x - x_guess
            #  print(f&#34;mean squared diff {i}:&#34;, (torch.sum(diff.pow(2))/diff.shape[1]).item())
            max_adj = (
                torch.heaviside(diff, torch.Tensor([0]).to(cum_y_guess)).clamp(
                    min=eps, max=1 - eps
                )
                - cum_y_guess
            )
            adj = max_adj * torch.tanh(diff.pow(2)) * lr

        density = self.prob_ito_cumprob(cum_y_guess)
        density = torch.nan_to_num(density, nan=0)
        return density

    def log_prob(self, x):
        &#39;&#39;&#39;Approximates log of probability density at a batch of tensors &#34;x&#34;.&#39;&#39;&#39;
        return torch.log(self.prob(x))

    def estimate_entropy(self, steps=256):
        &#39;&#39;&#39;Estimates shannon entropy of the distribution in nats by numeric integration.&#39;&#39;&#39;
        eps = 1e-7
        a = eps  # lower integration bound
        b = 1 - eps  # upper integration bound
        cum_y_tics = torch.Tensor(np.linspace(a, b, steps)).to(device)
        x_tics = self.quantile(cum_y_tics)
        p_tics = self.prob_ito_cumprob(cum_y_tics)
        entropy = -torch.trapz(p_tics*torch.log(p_tics), x_tics)
        return entropy

    def sample(self, shape: torch.Tensor.shape):
        &#39;&#39;&#39;Simulates data by inverse tranform.&#39;&#39;&#39;
        eps = 1e-7
        return self.quantile(torch.rand(shape).clamp(min=eps, max=1-eps))

    def forward(self, x):
        &#39;&#39;&#39;
        By default: Approximates probability density at a batch of tensors &#34;x&#34; by asymptotically
        bounded approach. There is currently no known closed-form inverse metalog.
        &#39;&#39;&#39;
        return self.prob(x)

def Metalog_Fit_Closed_Form(model, data):
    &#34;&#34;&#34;
    Fits the parameters of the metalog model, &#34;model&#34;, to sources of data in &#34;data&#34;, by a closed-form
    linear least-squares method.
    This function supports batching for fitting many datasets at once and expects data in batched
    form (with at least 2 dimensional shape). First dimension of data must match first dimension of
    model coefficients &#34;a&#34;. If first dimension &gt; 1 (batchsize &gt; 1), this function will fit a number
    of sets of coefficients, namely: one set of coefficients in the provided metalog model for each
    dataset, where the first-dimension or &#34;batch-size&#34; of &#34;data&#34; indicates the number of independent
    datasets.
    &#34;&#34;&#34;
    ecdf = ECDF(data, dim=1, reach_limits=False)
    x, y = ecdf
    x = x.float()
    y = y.float()

    Y_cols = [f(y, idx) for idx, f in enumerate(model.qf_basis_functions)]
    Y = torch.stack(Y_cols, -1)
    a = torch.bmm(
        torch.linalg.solve(torch.bmm(Y.transpose(1, 2), Y), Y.transpose(1, 2)),
        x.unsqueeze(-1),
    ).flatten(1)
    model.a.data = a</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="infotorch.ECDF"><code class="name flex">
<span>def <span class="ident">ECDF</span></span>(<span>x: torch.Tensor, dim: int = 0, reach_limits=True)</span>
</code></dt>
<dd>
<div class="desc"><p>set "reach_limit" to false to calculate ECDF in a way that will not include perfect 0 or 1.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ECDF(x: torch.Tensor, dim: int = 0, reach_limits=True):
    &#34;&#34;&#34;
    set &#34;reach_limit&#34; to false to calculate ECDF in a way that will not include perfect 0 or 1.
    &#34;&#34;&#34;
    x = torch.sort(x.flatten(dim), dim=dim).values
    n = x.shape[-1]
    cum = torch.arange(1, n + 1).to(x.device) / (n + 1 - reach_limits)
    cum = cum.repeat(*x.shape[0:-1], 1)  # one for each univariate sample
    return x, cum</code></pre>
</details>
</dd>
<dt id="infotorch.KernelDensityEstimate"><code class="name flex">
<span>def <span class="ident">KernelDensityEstimate</span></span>(<span>data, x_tics=None, start=-9, end=9, kernel=Normal(loc: 0.0, scale: 1.0), bandwidth_adjustment=1, dim=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimates the probability density function of a batch of data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def KernelDensityEstimate(
    data,
    x_tics=None,
    start=-9,
    end=9,
    kernel=Normal(loc=0, scale=1),
    bandwidth_adjustment=1,
    dim=1,
):
    &#39;&#39;&#39;Estimates the probability density function of a batch of data.&#39;&#39;&#39;
    # convert to positive index (important for unsqueezing)
    if dim &lt; 0:
        dim = len(data.shape) + dim
        if dim &gt; (len(data.shape) - 1) or dim &lt; 0:
            raise IndexError

    def kde_prob(
        data,
        x,
        dim=dim,
        kernel=Normal(loc=0, scale=1),
        bandwidth_adjustment=1,
    ):
        &#39;&#39;&#39;
        Returns the probability of the items in tensor &#39;x&#39; according to the PDF estimated by a KDE.
        This function is memory intensive.
        &#39;&#39;&#39;
        data = data.flatten(dim)
        n = data.shape[dim]
        silvermans_factor = ((4 * torch.std(data, dim).pow(5)) / (3 * n)).pow(1 / 5)

        bw = silvermans_factor * bandwidth_adjustment

        bw = bw.view(
            *bw.shape, *[1 for _ in range(len(data.shape) + 1 - len(bw.shape))]
        )
        a = data.unsqueeze(dim) - x.unsqueeze(1)
        a = a / bw
        a = kernel.log_prob(a)
        a = torch.exp(a)
        a = bw ** (-1) * a
        a = a.sum(dim=dim + 1)
        prob = a / n

        return prob

    if x_tics is None:
        assert start and end
        assert end &gt; start
        a = max(torch.min(data).item(), start)  # lower integration bound
        b = min(torch.max(data).item(), end)  # upper integration bound
        x_tics = torch.Tensor(np.linspace(a, b, steps)).to(data.device)
    else:
        assert isinstance(x_tics, torch.Tensor)
        x_tics = x_tics.to(data.device)
    x_tics.requires_grad = True
    kde_y_tics = kde_prob(
        data,
        x_tics,
        kernel=kernel,
        bandwidth_adjustment=bandwidth_adjustment,
    )
    return kde_y_tics</code></pre>
</details>
</dd>
<dt id="infotorch.MLE_Fit"><code class="name flex">
<span>def <span class="ident">MLE_Fit</span></span>(<span>model, data, dim=1, lr=0.05, iters=250)</span>
</code></dt>
<dd>
<div class="desc"><p>Fits the parameters of the provided model to the provided data. Provided model must have
implimented log_prob() and constrain() methods, and paraters set to some initial value.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def MLE_Fit(model, data, dim=1, lr=5e-2, iters=250):
    &#39;&#39;&#39;
    Fits the parameters of the provided model to the provided data. Provided model must have
    implimented log_prob() and constrain() methods, and paraters set to some initial value.
    &#39;&#39;&#39;
    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)
    #  print(&#34;model parameters:&#34;, [x for x in model.parameters()])
    #  data = data.flatten(dim)
    for i in range(iters):
        nll = -torch.sum(model.log_prob(data))
        nll.backward()
        optimizer.step()
        optimizer.zero_grad()
        model.constrain()</code></pre>
</details>
</dd>
<dt id="infotorch.Metalog_Fit_Closed_Form"><code class="name flex">
<span>def <span class="ident">Metalog_Fit_Closed_Form</span></span>(<span>model, data)</span>
</code></dt>
<dd>
<div class="desc"><p>Fits the parameters of the metalog model, "model", to sources of data in "data", by a closed-form
linear least-squares method.
This function supports batching for fitting many datasets at once and expects data in batched
form (with at least 2 dimensional shape). First dimension of data must match first dimension of
model coefficients "a". If first dimension &gt; 1 (batchsize &gt; 1), this function will fit a number
of sets of coefficients, namely: one set of coefficients in the provided metalog model for each
dataset, where the first-dimension or "batch-size" of "data" indicates the number of independent
datasets.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Metalog_Fit_Closed_Form(model, data):
    &#34;&#34;&#34;
    Fits the parameters of the metalog model, &#34;model&#34;, to sources of data in &#34;data&#34;, by a closed-form
    linear least-squares method.
    This function supports batching for fitting many datasets at once and expects data in batched
    form (with at least 2 dimensional shape). First dimension of data must match first dimension of
    model coefficients &#34;a&#34;. If first dimension &gt; 1 (batchsize &gt; 1), this function will fit a number
    of sets of coefficients, namely: one set of coefficients in the provided metalog model for each
    dataset, where the first-dimension or &#34;batch-size&#34; of &#34;data&#34; indicates the number of independent
    datasets.
    &#34;&#34;&#34;
    ecdf = ECDF(data, dim=1, reach_limits=False)
    x, y = ecdf
    x = x.float()
    y = y.float()

    Y_cols = [f(y, idx) for idx, f in enumerate(model.qf_basis_functions)]
    Y = torch.stack(Y_cols, -1)
    a = torch.bmm(
        torch.linalg.solve(torch.bmm(Y.transpose(1, 2), Y), Y.transpose(1, 2)),
        x.unsqueeze(-1),
    ).flatten(1)
    model.a.data = a</code></pre>
</details>
</dd>
<dt id="infotorch.bimodality_index"><code class="name flex">
<span>def <span class="ident">bimodality_index</span></span>(<span>x, dim=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Used to detect bimodality (or multimodality) of dataset(s) given a tensor "x" containing the
data and a dimension "dim" along which to calculate.
The logic behind this index is that a
bimodal (or multimodal) distribution with light tails will have very low kurtosis, an asymmetric
character, or both – all of which increase this index.
The smaller this value is the more
likely the data are to follow a unimodal distribution.
As a rule: if return value ≤ 0.555
(bimodal index for uniform distribution), the data are considered to follow a unimodal
distribution. Otherwise, they follow a bimodal or multimodal distribution.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bimodality_index(x, dim=1):
    &#39;&#39;&#39;
    Used to detect bimodality (or multimodality) of dataset(s) given a tensor &#34;x&#34; containing the
    data and a dimension &#34;dim&#34; along which to calculate.  The logic behind this index is that a
    bimodal (or multimodal) distribution with light tails will have very low kurtosis, an asymmetric
    character, or both – all of which increase this index.  The smaller this value is the more
    likely the data are to follow a unimodal distribution.  As a rule: if return value ≤ 0.555
    (bimodal index for uniform distribution), the data are considered to follow a unimodal
    distribution. Otherwise, they follow a bimodal or multimodal distribution.
    &#39;&#39;&#39;
    # calculate standard deviation and mean of dataset(s)
    std, mean = torch.std_mean(x, dim)
    # get number of samples in dataset(s)
    n = torch.Tensor([x.shape[dim]]).to(x.device)
    eps = 1e-6  # for stability

    # calculate skewness:
    # repeating most of the skewness function here to avoid recomputation of standard devation and mean
    sample_bias_adjustment = torch.sqrt(n * (n - 1)) / (n - 2)
    skew = sample_bias_adjustment * (
        (torch.sum((x.T - mean.unsqueeze(dim).T).T.pow(3), dim) / n)
        / std.pow(3).clamp(min=eps)
    )

    # calculate kurtosis:
    # repeating most the kurtosis function here to avoid recomputation of standard devation and mean
    sample_bias_adjustment = (n - 1) / ((n - 2) * (n - 3))
    kurt = sample_bias_adjustment * (
        (n + 1)
        * (
            (torch.sum((z.T - mean.unsqueeze(dim).T).T.pow(4), dim) / n)
            / std.pow(4).clamp(min=eps)
        )
        - 3 * (n - 1)
    )

    # calculate bimodality index:
    BC = (skew.pow(2) + 1) / (kurt + 3 * ((n - 2).pow(2) / ((n - 2) * (n - 3))))

    return BC</code></pre>
</details>
</dd>
<dt id="infotorch.kurtosis_fn"><code class="name flex">
<span>def <span class="ident">kurtosis_fn</span></span>(<span>x, dim=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates kurtosis of data "x" along dimension "dim".</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kurtosis_fn(x, dim=1):
    &#39;&#39;&#39;Calculates kurtosis of data &#34;x&#34; along dimension &#34;dim&#34;.&#39;&#39;&#39;
    std, mean = torch.std_mean(x, dim)
    n = torch.Tensor([x.shape[dim]]).to(x.device)
    eps = 1e-6  # for stability

    sample_bias_adjustment = (n - 1) / ((n - 2) * (n - 3))
    kurtosis = sample_bias_adjustment * (
        (n + 1)
        * (
            (torch.sum((x.T - mean.unsqueeze(dim).T).T.pow(4), dim) / n)
            / std.pow(4).clamp(min=eps)
        )
        - 3 * (n - 1)
    )
    return kurtosis</code></pre>
</details>
</dd>
<dt id="infotorch.skewness_fn"><code class="name flex">
<span>def <span class="ident">skewness_fn</span></span>(<span>x, dim=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates skewness of data "x" along dimension "dim".</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def skewness_fn(x, dim=1):
    &#39;&#39;&#39;Calculates skewness of data &#34;x&#34; along dimension &#34;dim&#34;.&#39;&#39;&#39;
    std, mean = torch.std_mean(x, dim)
    n = torch.Tensor([x.shape[dim]]).to(x.device)
    eps = 1e-6  # for stability

    sample_bias_adjustment = torch.sqrt(n * (n - 1)) / (n - 2)
    skewness = sample_bias_adjustment * (
        (torch.sum((x.T - mean.unsqueeze(dim).T).T.pow(3), dim) / n)
        / std.pow(3).clamp(min=eps)
    )
    return skewness</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="infotorch.Normal_Model"><code class="flex name class">
<span>class <span class="ident">Normal_Model</span></span>
<span>(</span><span>init_mean: torch.Tensor = tensor([0.]), init_std: torch.Tensor = tensor([1.]))</span>
</code></dt>
<dd>
<div class="desc"><p>Example of a module for modeling a probability distribution. This is set up with all pieces
required for use with the rest of this package. (initial parameters; as well as implimented
constrain, forward, and log_prob methods)</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Normal_Model(nn.Module):
    &#39;&#39;&#39;
    Example of a module for modeling a probability distribution. This is set up with all pieces
    required for use with the rest of this package. (initial parameters; as well as implimented
    constrain, forward, and log_prob methods)
    &#39;&#39;&#39;
    def __init__(
        self,
        init_mean: torch.Tensor = torch.Tensor([0]),
        init_std: torch.Tensor = torch.Tensor([1]),
    ):
        super(Normal_Model, self).__init__()
        self.mean = nn.Parameter(init_mean, requires_grad=True)
        self.std = nn.Parameter(init_std, requires_grad=True)
        # constant
        self.ln2p = nn.Parameter(
            torch.log(2 * torch.Tensor([torch.pi])), requires_grad=False
        )

    def constrain(self):
        &#39;&#39;&#39;
        Method to run on &#34;constrain&#34; step of training. Easiest method for optimization under
        constraint is Projection Optimization by simply clamping parameters to bounds after each
        update. This is certainly not the most efficent way, but it gets the job done.
        &#39;&#39;&#39;
        #  can&#39;t have negative standard deviation so lets prevent that:
        eps = 1e-6
        self.std.data = model.std.data.clamp(min=eps)

    def log_prob(self, x):
        &#39;&#39;&#39;
        Returns the log probability of the items in tensor &#39;x&#39; according to the probability
        distribution of the module.
        &#39;&#39;&#39;
        return (
            -torch.log(self.std.unsqueeze(-1))
            - (self.ln2p / 2)
            - ((x - self.mean.unsqueeze(-1)) / self.std.unsqueeze(-1)).pow(2) / 2
        )

    def forward(self, x):
        &#39;&#39;&#39;Returns the probability of the items in tensor &#39;x&#39; according to the probability distribution of the module.&#39;&#39;&#39;
        return self.log_prob(x).exp()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="infotorch.Normal_Model.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="infotorch.Normal_Model.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="infotorch.Normal_Model.constrain"><code class="name flex">
<span>def <span class="ident">constrain</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to run on "constrain" step of training. Easiest method for optimization under
constraint is Projection Optimization by simply clamping parameters to bounds after each
update. This is certainly not the most efficent way, but it gets the job done.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def constrain(self):
    &#39;&#39;&#39;
    Method to run on &#34;constrain&#34; step of training. Easiest method for optimization under
    constraint is Projection Optimization by simply clamping parameters to bounds after each
    update. This is certainly not the most efficent way, but it gets the job done.
    &#39;&#39;&#39;
    #  can&#39;t have negative standard deviation so lets prevent that:
    eps = 1e-6
    self.std.data = model.std.data.clamp(min=eps)</code></pre>
</details>
</dd>
<dt id="infotorch.Normal_Model.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the probability of the items in tensor 'x' according to the probability distribution of the module.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    &#39;&#39;&#39;Returns the probability of the items in tensor &#39;x&#39; according to the probability distribution of the module.&#39;&#39;&#39;
    return self.log_prob(x).exp()</code></pre>
</details>
</dd>
<dt id="infotorch.Normal_Model.log_prob"><code class="name flex">
<span>def <span class="ident">log_prob</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the log probability of the items in tensor 'x' according to the probability
distribution of the module.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_prob(self, x):
    &#39;&#39;&#39;
    Returns the log probability of the items in tensor &#39;x&#39; according to the probability
    distribution of the module.
    &#39;&#39;&#39;
    return (
        -torch.log(self.std.unsqueeze(-1))
        - (self.ln2p / 2)
        - ((x - self.mean.unsqueeze(-1)) / self.std.unsqueeze(-1)).pow(2) / 2
    )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="infotorch.Unbounded_Metalog_Model"><code class="flex name class">
<span>class <span class="ident">Unbounded_Metalog_Model</span></span>
<span>(</span><span>init_a: torch.Tensor = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Unbounded_Metalog_Model(nn.Module):
    def __init__(
        self,
        init_a: torch.Tensor = None,
    ):
        super(Unbounded_Metalog_Model, self).__init__()

        self.a = nn.Parameter(init_a, requires_grad=True)
        self.n = self.a.shape[-1]

        ### Define basis functions for QF (quantile function):
        def qg1(y, i):
            &#39;&#39;&#39;first basis function&#39;&#39;&#39;
            return torch.ones_like(y)

        def qg2(y, i):
            &#39;&#39;&#39;second basis function&#39;&#39;&#39;
            return torch.log(y / (1 - y))

        def qg3(y, i):
            &#39;&#39;&#39;third basis function&#39;&#39;&#39;
            return (y - 0.5) * torch.log(y / (1 - y))

        def qg4(y, i):
            &#39;&#39;&#39;fourth basis function&#39;&#39;&#39;
            return y - 0.5

        def qgj_odd(y, j):
            &#39;&#39;&#39;nth odd basis function (after third)&#39;&#39;&#39;
            j += 1
            assert (j % 2 != 0) and (j &gt;= 5)
            return (y - 0.5).pow((j - 1) / 2)

        def qgj_even(y, j):
            &#39;&#39;&#39;nth even basis function (after fourth)&#39;&#39;&#39;
            j += 1
            assert (j % 2 == 0) and (j &gt;= 6)
            return torch.log(y / (1 - y)) * (y - 0.5).pow(j / 2 - 1)

        # Start QF basis functions:
        self.qf_basis_functions = [qg1, qg2, qg3, qg4]
        # Additional inverse cdf basis functions as needed:
        self.qf_basis_functions = self.qf_basis_functions + [
            qgj_odd if x % 2 == 0 else qgj_even for x in range(self.n - 4)
        ]
        # Trim as needed:
        self.qf_basis_functions = self.qf_basis_functions[: self.n]

        ### Define basis functions for inverse PDF in terms of cumulative probability
        ### (^ derivative of quantile function):
        def dqg1(y, i):
            &#39;&#39;&#39;first basis function&#39;&#39;&#39;
            return torch.zeros_like(y)

        def dqg2(y, i):
            &#39;&#39;&#39;second basis function&#39;&#39;&#39;
            return 1 / (y * (1 - y))

        def dqg3(y, i):
            &#39;&#39;&#39;third basis function&#39;&#39;&#39;
            return (y - 1 / 2) / (y * (1 - y)) + torch.log(y / (1 - y))

        def dqg4(y, i):
            &#39;&#39;&#39;fourth basis function&#39;&#39;&#39;
            return torch.ones_like(y)

        def dqgj_odd(y, j):
            &#39;&#39;&#39;nth odd basis function (after third)&#39;&#39;&#39;
            j += 1
            assert (j % 2 != 0) and (j &gt;= 5)
            return ((j - 1) / 2) * (y - 1 / 2).pow((j - 3) / 2)

        def dqgj_even(y, j):
            &#39;&#39;&#39;nth even basis function (after fourth)&#39;&#39;&#39;
            j += 1
            assert (j % 2 == 0) and (j &gt;= 6)
            return (y - 1 / 2).pow(j / 2 - 1) / (y * (1 - y)) + (j / 2 - 1) * (
                y - 1 / 2
            ).pow(j / 2 - 2) * torch.log(y / (1 - y))

        # Start PDF basis functions:
        self.ipdf_basis_functions = [dqg1, dqg2, dqg3, dqg4]
        # Additional ipdf basis functions as needed:
        self.ipdf_basis_functions = self.ipdf_basis_functions + [
            dqgj_odd if x % 2 == 0 else dqgj_even for x in range(self.n - 4)
        ]
        # Trim as needed:
        self.ipdf_basis_functions = self.ipdf_basis_functions[: self.n]

    def constrain(self):
        &#39;&#39;&#39;Coefficients are unconstrained in this case.&#39;&#39;&#39;
        pass

    def quantile(self, y):
        &#39;&#39;&#39;
        Quantile of cumulative probability &#34;y&#34;.  (returns x-position of cumulative probability &#34;y&#34;.
        This is an inverse CDF)
        &#39;&#39;&#39;
        x_tics = sum(
            [
                self.a[:, idx].unsqueeze(-1) * f(y, idx)
                for idx, f in enumerate(self.qf_basis_functions)
            ]
        )
        return x_tics

    def prob_ito_cumprob(self, y):
        &#39;&#39;&#39;Probability density in terms of cumulative probability &#34;y&#34;.&#39;&#39;&#39;
        return sum(
            [
                self.a[:, idx].unsqueeze(-1) * f(y, idx)
                for idx, f in enumerate(self.ipdf_basis_functions)
            ]
        ).pow(
            -1
        )  # for reciprocal of sum of terms

    def prob(self, x, iters=64):
        &#39;&#39;&#39;
        Approximates probability density at a batch of tensors &#34;x&#34; by asymptotically bounded
        approach. There is currently no known closed-form inverse metalog.
        &#39;&#39;&#39;
        eps = 1e-7
        cum_y_guess = torch.ones_like(x) * 1 / 3

        lr = 1/3
        old_x_guess = self.quantile(cum_y_guess)  # initial
        old_diff = 0  # initial
        adj = torch.tensor([1]).to(x.device) / x.shape[1]  # initial
        for i in range(iters):
            cum_y_guess += adj
            x_guess = self.quantile(cum_y_guess)
            diff = x - x_guess
            #  print(f&#34;mean squared diff {i}:&#34;, (torch.sum(diff.pow(2))/diff.shape[1]).item())
            max_adj = (
                torch.heaviside(diff, torch.Tensor([0]).to(cum_y_guess)).clamp(
                    min=eps, max=1 - eps
                )
                - cum_y_guess
            )
            adj = max_adj * torch.tanh(diff.pow(2)) * lr

        density = self.prob_ito_cumprob(cum_y_guess)
        density = torch.nan_to_num(density, nan=0)
        return density

    def log_prob(self, x):
        &#39;&#39;&#39;Approximates log of probability density at a batch of tensors &#34;x&#34;.&#39;&#39;&#39;
        return torch.log(self.prob(x))

    def estimate_entropy(self, steps=256):
        &#39;&#39;&#39;Estimates shannon entropy of the distribution in nats by numeric integration.&#39;&#39;&#39;
        eps = 1e-7
        a = eps  # lower integration bound
        b = 1 - eps  # upper integration bound
        cum_y_tics = torch.Tensor(np.linspace(a, b, steps)).to(device)
        x_tics = self.quantile(cum_y_tics)
        p_tics = self.prob_ito_cumprob(cum_y_tics)
        entropy = -torch.trapz(p_tics*torch.log(p_tics), x_tics)
        return entropy

    def sample(self, shape: torch.Tensor.shape):
        &#39;&#39;&#39;Simulates data by inverse tranform.&#39;&#39;&#39;
        eps = 1e-7
        return self.quantile(torch.rand(shape).clamp(min=eps, max=1-eps))

    def forward(self, x):
        &#39;&#39;&#39;
        By default: Approximates probability density at a batch of tensors &#34;x&#34; by asymptotically
        bounded approach. There is currently no known closed-form inverse metalog.
        &#39;&#39;&#39;
        return self.prob(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="infotorch.Unbounded_Metalog_Model.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="infotorch.Unbounded_Metalog_Model.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="infotorch.Unbounded_Metalog_Model.constrain"><code class="name flex">
<span>def <span class="ident">constrain</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Coefficients are unconstrained in this case.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def constrain(self):
    &#39;&#39;&#39;Coefficients are unconstrained in this case.&#39;&#39;&#39;
    pass</code></pre>
</details>
</dd>
<dt id="infotorch.Unbounded_Metalog_Model.estimate_entropy"><code class="name flex">
<span>def <span class="ident">estimate_entropy</span></span>(<span>self, steps=256)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimates shannon entropy of the distribution in nats by numeric integration.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def estimate_entropy(self, steps=256):
    &#39;&#39;&#39;Estimates shannon entropy of the distribution in nats by numeric integration.&#39;&#39;&#39;
    eps = 1e-7
    a = eps  # lower integration bound
    b = 1 - eps  # upper integration bound
    cum_y_tics = torch.Tensor(np.linspace(a, b, steps)).to(device)
    x_tics = self.quantile(cum_y_tics)
    p_tics = self.prob_ito_cumprob(cum_y_tics)
    entropy = -torch.trapz(p_tics*torch.log(p_tics), x_tics)
    return entropy</code></pre>
</details>
</dd>
<dt id="infotorch.Unbounded_Metalog_Model.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>By default: Approximates probability density at a batch of tensors "x" by asymptotically
bounded approach. There is currently no known closed-form inverse metalog.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    &#39;&#39;&#39;
    By default: Approximates probability density at a batch of tensors &#34;x&#34; by asymptotically
    bounded approach. There is currently no known closed-form inverse metalog.
    &#39;&#39;&#39;
    return self.prob(x)</code></pre>
</details>
</dd>
<dt id="infotorch.Unbounded_Metalog_Model.log_prob"><code class="name flex">
<span>def <span class="ident">log_prob</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Approximates log of probability density at a batch of tensors "x".</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_prob(self, x):
    &#39;&#39;&#39;Approximates log of probability density at a batch of tensors &#34;x&#34;.&#39;&#39;&#39;
    return torch.log(self.prob(x))</code></pre>
</details>
</dd>
<dt id="infotorch.Unbounded_Metalog_Model.prob"><code class="name flex">
<span>def <span class="ident">prob</span></span>(<span>self, x, iters=64)</span>
</code></dt>
<dd>
<div class="desc"><p>Approximates probability density at a batch of tensors "x" by asymptotically bounded
approach. There is currently no known closed-form inverse metalog.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prob(self, x, iters=64):
    &#39;&#39;&#39;
    Approximates probability density at a batch of tensors &#34;x&#34; by asymptotically bounded
    approach. There is currently no known closed-form inverse metalog.
    &#39;&#39;&#39;
    eps = 1e-7
    cum_y_guess = torch.ones_like(x) * 1 / 3

    lr = 1/3
    old_x_guess = self.quantile(cum_y_guess)  # initial
    old_diff = 0  # initial
    adj = torch.tensor([1]).to(x.device) / x.shape[1]  # initial
    for i in range(iters):
        cum_y_guess += adj
        x_guess = self.quantile(cum_y_guess)
        diff = x - x_guess
        #  print(f&#34;mean squared diff {i}:&#34;, (torch.sum(diff.pow(2))/diff.shape[1]).item())
        max_adj = (
            torch.heaviside(diff, torch.Tensor([0]).to(cum_y_guess)).clamp(
                min=eps, max=1 - eps
            )
            - cum_y_guess
        )
        adj = max_adj * torch.tanh(diff.pow(2)) * lr

    density = self.prob_ito_cumprob(cum_y_guess)
    density = torch.nan_to_num(density, nan=0)
    return density</code></pre>
</details>
</dd>
<dt id="infotorch.Unbounded_Metalog_Model.prob_ito_cumprob"><code class="name flex">
<span>def <span class="ident">prob_ito_cumprob</span></span>(<span>self, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Probability density in terms of cumulative probability "y".</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prob_ito_cumprob(self, y):
    &#39;&#39;&#39;Probability density in terms of cumulative probability &#34;y&#34;.&#39;&#39;&#39;
    return sum(
        [
            self.a[:, idx].unsqueeze(-1) * f(y, idx)
            for idx, f in enumerate(self.ipdf_basis_functions)
        ]
    ).pow(
        -1
    )  # for reciprocal of sum of terms</code></pre>
</details>
</dd>
<dt id="infotorch.Unbounded_Metalog_Model.quantile"><code class="name flex">
<span>def <span class="ident">quantile</span></span>(<span>self, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Quantile of cumulative probability "y".
(returns x-position of cumulative probability "y".
This is an inverse CDF)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def quantile(self, y):
    &#39;&#39;&#39;
    Quantile of cumulative probability &#34;y&#34;.  (returns x-position of cumulative probability &#34;y&#34;.
    This is an inverse CDF)
    &#39;&#39;&#39;
    x_tics = sum(
        [
            self.a[:, idx].unsqueeze(-1) * f(y, idx)
            for idx, f in enumerate(self.qf_basis_functions)
        ]
    )
    return x_tics</code></pre>
</details>
</dd>
<dt id="infotorch.Unbounded_Metalog_Model.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self, shape: <attribute 'shape' of 'torch._C._TensorBase' objects>)</span>
</code></dt>
<dd>
<div class="desc"><p>Simulates data by inverse tranform.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(self, shape: torch.Tensor.shape):
    &#39;&#39;&#39;Simulates data by inverse tranform.&#39;&#39;&#39;
    eps = 1e-7
    return self.quantile(torch.rand(shape).clamp(min=eps, max=1-eps))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="infotorch.ECDF" href="#infotorch.ECDF">ECDF</a></code></li>
<li><code><a title="infotorch.KernelDensityEstimate" href="#infotorch.KernelDensityEstimate">KernelDensityEstimate</a></code></li>
<li><code><a title="infotorch.MLE_Fit" href="#infotorch.MLE_Fit">MLE_Fit</a></code></li>
<li><code><a title="infotorch.Metalog_Fit_Closed_Form" href="#infotorch.Metalog_Fit_Closed_Form">Metalog_Fit_Closed_Form</a></code></li>
<li><code><a title="infotorch.bimodality_index" href="#infotorch.bimodality_index">bimodality_index</a></code></li>
<li><code><a title="infotorch.kurtosis_fn" href="#infotorch.kurtosis_fn">kurtosis_fn</a></code></li>
<li><code><a title="infotorch.skewness_fn" href="#infotorch.skewness_fn">skewness_fn</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="infotorch.Normal_Model" href="#infotorch.Normal_Model">Normal_Model</a></code></h4>
<ul class="">
<li><code><a title="infotorch.Normal_Model.constrain" href="#infotorch.Normal_Model.constrain">constrain</a></code></li>
<li><code><a title="infotorch.Normal_Model.dump_patches" href="#infotorch.Normal_Model.dump_patches">dump_patches</a></code></li>
<li><code><a title="infotorch.Normal_Model.forward" href="#infotorch.Normal_Model.forward">forward</a></code></li>
<li><code><a title="infotorch.Normal_Model.log_prob" href="#infotorch.Normal_Model.log_prob">log_prob</a></code></li>
<li><code><a title="infotorch.Normal_Model.training" href="#infotorch.Normal_Model.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="infotorch.Unbounded_Metalog_Model" href="#infotorch.Unbounded_Metalog_Model">Unbounded_Metalog_Model</a></code></h4>
<ul class="two-column">
<li><code><a title="infotorch.Unbounded_Metalog_Model.constrain" href="#infotorch.Unbounded_Metalog_Model.constrain">constrain</a></code></li>
<li><code><a title="infotorch.Unbounded_Metalog_Model.dump_patches" href="#infotorch.Unbounded_Metalog_Model.dump_patches">dump_patches</a></code></li>
<li><code><a title="infotorch.Unbounded_Metalog_Model.estimate_entropy" href="#infotorch.Unbounded_Metalog_Model.estimate_entropy">estimate_entropy</a></code></li>
<li><code><a title="infotorch.Unbounded_Metalog_Model.forward" href="#infotorch.Unbounded_Metalog_Model.forward">forward</a></code></li>
<li><code><a title="infotorch.Unbounded_Metalog_Model.log_prob" href="#infotorch.Unbounded_Metalog_Model.log_prob">log_prob</a></code></li>
<li><code><a title="infotorch.Unbounded_Metalog_Model.prob" href="#infotorch.Unbounded_Metalog_Model.prob">prob</a></code></li>
<li><code><a title="infotorch.Unbounded_Metalog_Model.prob_ito_cumprob" href="#infotorch.Unbounded_Metalog_Model.prob_ito_cumprob">prob_ito_cumprob</a></code></li>
<li><code><a title="infotorch.Unbounded_Metalog_Model.quantile" href="#infotorch.Unbounded_Metalog_Model.quantile">quantile</a></code></li>
<li><code><a title="infotorch.Unbounded_Metalog_Model.sample" href="#infotorch.Unbounded_Metalog_Model.sample">sample</a></code></li>
<li><code><a title="infotorch.Unbounded_Metalog_Model.training" href="#infotorch.Unbounded_Metalog_Model.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>